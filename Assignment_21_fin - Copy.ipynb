{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3fdb04",
   "metadata": {},
   "source": [
    "# Text Classification:\n",
    "\n",
    "## Data\n",
    "\n",
    "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
    "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
    "so from document name, you can extract the label for that document.\n",
    "4. Now our problem is to classify all the documents into any one of the class.\n",
    "5. Below we provided count plot of all the labels in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4221bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_device = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_device[0],True)\n",
    "len(physical_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0caceb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input,Dense,Conv1D,Flatten,Embedding,MaxPool1D,concatenate,Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60423a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "# !pip install tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "os.chdir(\"D:\\\\Applied_AI\\\\refrence for assignment 21\\\\documents\")\n",
    "files = glob.glob('*.txt')\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d329e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readlines(files):\n",
    "    lst = []\n",
    "    for fl in range(len(files)):\n",
    "        raw = open(files[fl]).readlines()\n",
    "        lst.append(raw)\n",
    "    return lst\n",
    "readlines_file = readlines(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17dae124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(files):\n",
    "    lst = []\n",
    "    for fl in range(len(files)):\n",
    "        raw = open(files[fl]).read()\n",
    "        lst.append(raw)\n",
    "    return lst\n",
    "read_file = read(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47bd57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []     #list contains Y_labels i.e text before'_'.\n",
    "for i in range(len(files)):\n",
    "    y = re.split('_',files[i])\n",
    "    label.append(y[0])\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e3a30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess0(files):    \n",
    "#     preprocessed_emails = []    #Step 1\n",
    "#     text_WO_mail = []    #Step 2\n",
    "\n",
    "#     for i in read(files):\n",
    "#         a = []\n",
    "#         filtered = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',i)\n",
    "#         modified = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',' ',i)\n",
    "#         text_WO_mail.append(modified)\n",
    "#         for j in filtered:\n",
    "#             domain = j.split('@')[1].split('.')\n",
    "#             for i in domain:\n",
    "#                 if i == 'com':\n",
    "#                     continue\n",
    "#                 if len(i) > 2:\n",
    "#                     a.append(i)\n",
    "#         preprocessed_emails.append(' '.join(a))\n",
    "#     return preprocessed_emails,text_WO_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e07bce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(txt):\n",
    "    emails = []\n",
    "    a = []\n",
    "    \n",
    "    filtered = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',txt)\n",
    "    for j in filtered:\n",
    "        domain = j.split('@')[1].split('.')\n",
    "        for i in domain:\n",
    "            if i == 'com':\n",
    "                continue\n",
    "            if len(i) > 2:\n",
    "                a.append(i)\n",
    "    emails.append(' '.join(a))\n",
    "    return emails.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a28e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sub(txt):\n",
    "    for i in txt:\n",
    "        if 'Subject:' in i:\n",
    "            sub = i.split(':')[-1]\n",
    "            sub1 = re.sub('\\!|\\\"|\\#|\\$|\\%|\\&|\\\\|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\<|\\=|\\>|\\?|\\@|\\[|\\]|\\^|\\_|\\`|\\{|\\}|\\~|\\n|\\t','',sub)\n",
    "    return sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "675039f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_mail(txt):\n",
    "    return re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc0b6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_sub(txt):\n",
    "    return re.sub(r'Subject:[^\\n]+',' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ff4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_From(txt):\n",
    "    return re.sub(r'Write to:|From:[^\\n]+','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26189d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_tag(txt):\n",
    "    return re.sub(\"<[^>]*>\", \"\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9977643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_bracket(txt):\n",
    "    return re.sub(r'\\([^)]*\\)|(<.*>)',' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0573e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_Newlines(txt):\n",
    "    return re.sub(r'\\n|\\t|\\-|\\\\', '',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff505b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_colon(txt):\n",
    "    return re.sub(r'[a-zA-Z]+:','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0fe7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract(txt):\n",
    "    a = re.sub(r\"won't\", \"will not\", txt)\n",
    "    a = re.sub(r\"can\\'t\", \"can not\", txt)\n",
    "    a = re.sub(r\"n\\'t\", \" not\",txt)\n",
    "    a = re.sub(r\"\\'re\", \" are\", txt)\n",
    "    a = re.sub(r\"\\'s\", \" is\", txt)\n",
    "    a = re.sub(r\"\\'d\", \" would\", txt)\n",
    "    a = re.sub(r\"\\'ll\", \" will\", txt)\n",
    "    a = re.sub(r\"\\'t\", \" not\", txt)\n",
    "    a = re.sub(r\"\\'ve\", \" have\", txt)\n",
    "    a = re.sub(r\"\\'m\", \" am\", txt)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e81e6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b13e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "def chunk(txt):\n",
    "#     chunks = []\n",
    "    chunks = (list(ne_chunk(pos_tag(word_tokenize(txt)))))   \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7acea2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 11 contd \n",
    "def combining_and_filtering(file, chunks):\n",
    "    for i in chunks:\n",
    "        if type(i)==Tree:\n",
    "            if i.label() == \"GPE\":\n",
    "                j = i.leaves()\n",
    "                if len(j)>1:   #if new_delhi or bigger name\n",
    "                    gpe = \"_\".join([term for term,pos in j])\n",
    "                    f = re.sub(rf'{j[1][0]}',gpe,file, flags=re.MULTILINE)              #replacing delhi with new_delhi\n",
    "                    file = re.sub(rf'\\b{j[0][0]}\\b',\"\",f, flags=re.MULTILINE)       #deleting new, \\b is important\n",
    "            if i.label() == \"PERSON\":           # deleting Ramesh         \n",
    "                for term,pos in i.leaves():\n",
    "                    file = re.sub(re.escape(term),\"\",file, flags=re.MULTILINE)\n",
    "\n",
    "    return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "374b1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_num(txt):\n",
    "    return re.sub( r\"[0-9]\",' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b5eb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_uScore(txt):\n",
    "    return re.sub(r\"\\b_([a-zA-z]+)_\\b|\\b_([a-zA-z]+)\\b|\\b([a-zA-z]+)_\\b\",r\"\\1\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "039ad73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_smallwords(txt):\n",
    "    return re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)|\\b[a-zA-Z]{2}_([a-zA-Z]+)\",r\"\\1\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ba908cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(txt):\n",
    "    file_lowr = txt.lower()\n",
    "    a1 = re.sub(r'\\b\\w{1,2}\\b',\" \",file_lowr)\n",
    "    aa = re.sub(r\"\\b\\w{15,}\\b\",\" \",a1)\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2833e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WO_space(txt):\n",
    "    txt_format = r\"[^a-zA-Z_]\"\n",
    "    ab = re.sub(txt_format, \" \",txt)\n",
    "    aa = re.sub(r\" {2,}\", \" \", ab, flags=re.MULTILINE)\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17ca7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(files):\n",
    "    \"\"\"Do all the Preprocessing as shown above and\n",
    "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
    "    preprocess_text = []\n",
    "    preprocessed_emails = []\n",
    "    preprocess_subject = []\n",
    "    for fl in range(len(files)):\n",
    "        raw = open(files[fl]).read()\n",
    "        raw_ = open(files[fl]).readlines()\n",
    "        preprocessed_emails.append(process_email(raw))\n",
    "#         for txt in files:\n",
    "        a = WO_sub(raw)\n",
    "        b = WO_From(a)\n",
    "        c = WO_Newlines(b)\n",
    "        d = WO_colon(c)\n",
    "        e = decontract(d)\n",
    "        f = chunk(e)\n",
    "        g = combining_and_filtering(e,f)\n",
    "        h = WO_num(g)\n",
    "        i = WO_uScore(h)\n",
    "        j = WO_smallwords(i)\n",
    "        k = lower_case(j)\n",
    "        l = WO_space(k)\n",
    "        m = WO_mail(l)\n",
    "        preprocess_text.append(m)\n",
    "        preprocess_subject.append(process_sub(raw_))\n",
    "\n",
    "    return preprocessed_emails,preprocess_text,preprocess_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c2eba0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' atheism december atheist resources addresses atheist organizations usafreedom from religion foundation fish bumper stickers and assorted other atheist paraphernalia areavailable from the freedom from religion foundation the ffrf box evolution signs sell the fish fish symbol like the oness stick their cars but with feet and the word writteninside the deluxe moulded plastic fish postpaid the people the san francisco bay area can get from try mailing figmo netcom com for net people who directly theprice per fish american atheist pressaap publish various atheist books critiques the bible lists ofbiblical contradictions and one such book the bible handbook and american atheist isbn edition bible contradictions absurdities atrocities immoralities contains the itself aap based the king version the bible american atheist box austin austin prometheus bookssell books including see below east amherst street new_new_york alternate address which may newer older prometheus for humanisman organization promoting black secular humanism and uncovering the history ofblack freethought they publish quarterly newsletter aah examine african americans for humanism box association national secular society street holloway london london british humanist association south place ethical society lamb conduit passage conway halllondon red lion square london rlfax the national secular society publish the freethinker monthly germanyibka bund der und postfach germany ibka publish miz materialien und zur zeit der und ibka mizvertrieb postfach germany for atheist books write ibdk ucherdienst der germany fictionthomas disch the claus compromise short story the ultimate proof that exists all characters and events are fictitious any similarity living dead gods well walter miller canticle for leibowitz one gem this post atomic doomsday novel the monks who spent their livescopying blueprints from saint leibowitz filling the sheets paper withink and leaving white lines and letters edgar pangborn post atomic doomsday novel set clerical states the church for example forbids that anyone produce describe use any substance containing atoms philip dickphilip wrote many philosophical and short stories and novels his stories are bizarre times but very approachable wrote mainly but wrote about people truth and religion rather thantechnology although often believed that had met some sort heremained sceptical amongst his novels the following are some galactic pothealer fallible alien deity summons group craftsmen and women aremote planet raise giant cathedral from beneath the oceans when thedeity begins demand faith from the earthers pothealer isunable comply polished ironic and amusing novel maze death noteworthy for its description religion valis the schizophrenic hero searches for the hidden mysteries gnosticity after reality fired into his brain pink laser beam ofunknown but possibly divine origin accompanied his dogmatic atheist friend and assorted other odd characters the divine invasion invades making young woman pregnant she returns fromanother star system unfortunately she terminally ill and must beassisted dead man whose brain wired hour easy listening music margaret atwood the handmaid tale story based the premise that the congress and quickly take charge the nation set right again the book the diary woman life she tries liveunder the new theocracy right own property revoked and their bank accounts are closed sinful luxuries are outlawed and theradio only used for readings from the bible crimes are doctors who performed legal abortions the old world arehunted down and hanged writing style difficult get used toat first but the tale grows more and more chilling goes various authors the bible this somewhat dull and rambling work has often been criticized however itis probably worth reading only that you know what all the fuss isabout exists many different versions make sure you get the onetrue version rosa vicars christ although seems even catholic this veryenlighting history papal immoralities adulteries fallacies etc german gottes erste dunkle seite des droemerknaur michael martin philosophical justification philadelphia usa detailed and scholarly justification atheism contains defining terminology and usage this necessarily argues both for negative atheism the nonbelief theexistence god and also for positive atheism the belief god includes great refutations the arguments for god particular attention paid theists such and swinburne pages isbn hardcover paperback also available the case against ity comprehensive critique ity which considersthe best contemporary defences ity and ultimately demonstrates that they are unsupportable and incoherent pages isbn james turner the johns hopkins baltimore usasubtitled the origins unbelief america examines the way whichunbelief whether agnostic atheistic became mainstream focusses the period and while considering franceand britain the emphasis american and particularly neither religious history secularization atheism rather the intellectual history the fateof single idea the belief that exists pages isbn hardcover paper george sels editor the great thoughts antine new_new_york usaa dictionary quotations different kind concentrating statementsand writings which explicitly implicitly present the person philosophyand worldview includes obscure and often suppressed opinions from manypeople for some popular observations traces the way which variouspeople expressed and twisted the idea over the centuries quite number ofthe quotations are derived from cardiff what religion and views religion pages isbn paper richard swinburne the existence revised edition oxfordthis book the second volume trilogy that began with the coherence oftheism and was concluded with and thiswork swinburne attempts construct series inductive arguments for theexistence his arguments which are somewhat tendentious and relyupon the imputation late century western values andaesthetics which supposedly simple can conceived weredecisively rejected mackie the miracle theism the revisededition the existence swinburne includes appendix which hemakes somewhat incoherent attempt rebut mackie mackie the miracle theism oxfordthis posthumous volume contains comprehensive review the for and against the existence ranges from the positions descartes anselm throughthe moral arguments newman kant and the recent restatementsof the classical theses and swinburne also addresses thosepositions which push the concept beyond the realm the rational such those kierkegaard and well replacements for such axiarchism the book delight read and better written than works and refreshingly directwhen compared with the handwaving swinburne james haught illustrated history religious murder and madness prometheus looks religious persecution from ancient times the present day andnot only library congress catalog card number norm allen african american anthology see the listing for african americans for humanism above gordon stein anthology atheism and rationalism prometheus anthology covering wide range subjects including the devil and the history freethought comprehensive bibliography edmund cohen the mind the biblebeliever prometheus study why people become and what effect ithas them net resourcesthere small mailbased archive server mantis which old alt atheism moderated articles and assorted other files formore information send mail archiveserver mantis saying help send atheism indexand will mail back reply mathew ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_mail,p_text,p_sub, = preprocess(['alt.atheism_49960.txt'])\n",
    "p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be6cf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mail, p_text, p_subject = preprocess(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92df26c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18828, 18828, 18828)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_mail),len(p_text),len(p_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c31c9050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_mail</th>\n",
       "      <th>preprocessed_subject</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mantis netcom mantis</td>\n",
       "      <td>Atheist Resources</td>\n",
       "      <td>atheism december atheist resources addresses ...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mantis mantis mantis</td>\n",
       "      <td>Introduction to Atheism</td>\n",
       "      <td>atheism pril egin pgp signed messge introduct...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbstu1 tu-bs mimsy umd edu umd edu</td>\n",
       "      <td>Gospel Dating</td>\n",
       "      <td>article mimsy umd edu mangoe umd edu well has...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mantis kepler unh edu</td>\n",
       "      <td>university violating separation of churchstate</td>\n",
       "      <td>dmn kepler unh edu until kings become philoso...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Watson Ibm Com harder ccr-p ida org harder ccr...</td>\n",
       "      <td>socmotss et al Princeton axes matching funds ...</td>\n",
       "      <td>article apr harder ccrp ida org harder ccrp i...</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   preprocessed_mail  \\\n",
       "0                               mantis netcom mantis   \n",
       "1                               mantis mantis mantis   \n",
       "2                 dbstu1 tu-bs mimsy umd edu umd edu   \n",
       "3                              mantis kepler unh edu   \n",
       "4  Watson Ibm Com harder ccr-p ida org harder ccr...   \n",
       "\n",
       "                                preprocessed_subject  \\\n",
       "0                                  Atheist Resources   \n",
       "1                            Introduction to Atheism   \n",
       "2                                      Gospel Dating   \n",
       "3     university violating separation of churchstate   \n",
       "4   socmotss et al Princeton axes matching funds ...   \n",
       "\n",
       "                                   preprocessed_text        class  \n",
       "0   atheism december atheist resources addresses ...  alt.atheism  \n",
       "1   atheism pril egin pgp signed messge introduct...  alt.atheism  \n",
       "2   article mimsy umd edu mangoe umd edu well has...  alt.atheism  \n",
       "3   dmn kepler unh edu until kings become philoso...  alt.atheism  \n",
       "4   article apr harder ccrp ida org harder ccrp i...  alt.atheism  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'preprocessed_mail':p_mail,'preprocessed_subject':p_subject,'preprocessed_text':p_text,'class':label}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "# X = pd.DataFrame(joined)\n",
    "# y = label\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89f0ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = []\n",
    "for i in range(len(p_mail)):\n",
    "    a = [p_subject[i],p_mail[i],p_text[i]]\n",
    "    joined.append(\" \".join(a))\n",
    "# joined[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "41c6263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00f5ece5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18828, 1), (18828,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(joined)\n",
    "y = df['class']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "702d8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "## encoding lables \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoder_y = encoder.transform(y)\n",
    "## converting it to a matrix \n",
    "y = np_utils.to_categorical(encoder_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6970237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18828, 1), (18828, 20))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c18911e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14121, 1), (14121, 20), (4707, 1), (4707, 20))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=82)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8b6e36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,    19,    21, ..., 43920, 46730, 54391])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len = [len(i) for i in X_tr[0]]\n",
    "sent_len.sort()\n",
    "sent_len = np.array(sent_len)\n",
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d553fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4455,  5694,  8942, 54391])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles = []\n",
    "for i in range(97,101):\n",
    "    a = int(np.percentile(sent_len,i))\n",
    "    percentiles.append(a)\n",
    "np.array(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c7cba733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4089"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_98 = int(np.percentile(sent_len,98))\n",
    "percentile_981 = int(np.percentile(sent_len,98.1))\n",
    "percentile_982 = int(np.percentile(sent_len,98.2))\n",
    "percentile_983 = int(np.percentile(sent_len,98.3))\n",
    "percentile_984 = int(np.percentile(sent_len,98.4))\n",
    "percentile_985 = int(np.percentile(sent_len,98.5))\n",
    "percentile_986 = int(np.percentile(sent_len,98.6))\n",
    "percentile_987 = int(np.percentile(sent_len,98.7))\n",
    "percentile_988 = int(np.percentile(sent_len,98.8))\n",
    "percentile_989 = int(np.percentile(sent_len,96.9))\n",
    "\n",
    "\n",
    "percentile_98,percentile_981,percentile_982,percentile_983,percentile_984,percentile_985,percentile_986,percentile_987,percentile_988,percentile_989,\n",
    "percentile_989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7bb1018c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   11,    21,    21, ..., 38044, 40217, 47958])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len = [len(i) for i in X_te[0]]\n",
    "sent_len.sort()\n",
    "sent_len = np.array(sent_len)\n",
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "905dacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    document_count=0,\n",
    ")\n",
    "tokenizer.fit_on_texts(X_tr[0])\n",
    "X_tr1  = tokenizer.texts_to_sequences(X_tr[0])\n",
    "X_te1 = tokenizer.texts_to_sequences(X_te[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0a49e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr2 = pad_sequences(X_tr1, maxlen=percentile_989, padding=\"post\")\n",
    "X_te2= pad_sequences(X_te1, maxlen=percentile_989, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "07a4d203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14121, 4089), (4707, 4089), (14121, 20), (4707, 20))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr2.shape, X_te2.shape, y_tr.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c362f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:\\Applied_AI\\refrence for assignment 21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5ed69941",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = {}\n",
    "pretrain = open(\"glove.6B.50d.txt\", encoding=\"utf8\") \n",
    "for i in pretrain:\n",
    "    value = i.split(\" \")\n",
    "    word = value[0]\n",
    "    vector = np.asarray(value[1:])\n",
    "    embedded_dict[word] = vector\n",
    "pretrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5aacd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting embedding word to embedding matrix \n",
    "import numpy as np \n",
    "\n",
    "size = len(tokenizer.word_index)+1\n",
    "emb_matrix = np.zeros((size, 50))\n",
    "for word, a in tokenizer.word_index.items():\n",
    "    emb_word = embedded_dict.get(word)\n",
    "\n",
    "    if emb_word is not None:\n",
    "      emb_matrix[a] = emb_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b79fb",
   "metadata": {},
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "21a277cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embeding layer\n",
    "embedding_layer = Embedding(len(tokenizer.word_index)+1, 50,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(emb_matrix),\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e60189de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining layer\n",
    "first_layer = Input(shape=(percentile_989))\n",
    "\n",
    "\n",
    "embed = embedding_layer(first_layer)\n",
    "## hidden lyers\n",
    "m1 = Conv1D(32,4,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed)\n",
    "n1 = Conv1D(64,4,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed)\n",
    "o1 = Conv1D(128,4,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed)\n",
    "\n",
    "second_layer = concatenate([m1,n1,o1])\n",
    "\n",
    "max_pool_1 = MaxPool1D(3)(second_layer)\n",
    "\n",
    "\n",
    "i1 = Conv1D(32,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "j1 = Conv1D(32,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "k1 =  Conv1D(8,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "\n",
    "\n",
    "third_layer = concatenate([i1,j1,k1])\n",
    "dropout_layer1 = Dropout(0.2)(third_layer)\n",
    "\n",
    "max_pool_2 = MaxPool1D(3)(dropout_layer1)\n",
    "\n",
    "##adding another layer\n",
    "# i2 = Conv1D(128,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_2)\n",
    "# j2 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_2)\n",
    "# k2 =  Conv1D(32,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_2)\n",
    "\n",
    "# fourth_layer = concatenate([i2,j2,k2])\n",
    "\n",
    "# max_pool_3 = MaxPool1D(3)(fourth_layer)\n",
    "\n",
    "\n",
    "# fifth_layer = Conv1D(32,3,activation='relu',kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_3)\n",
    "fifth_layer = Conv1D(64,3,activation='relu',kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_2)\n",
    "\n",
    "flatten = Flatten()(fifth_layer)\n",
    "\n",
    "dropout_layer = Dropout(0.2)(flatten)\n",
    "\n",
    "dense_layer = Dense(64,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal())(dropout_layer)\n",
    "              \n",
    "output_layer = Dense(20,activation=\"softmax\",kernel_initializer= tf.keras.initializers.glorot_normal())(dense_layer)\n",
    "              \n",
    "\n",
    "model =Model(inputs=first_layer,outputs=output_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3b15e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 4089)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4089, 50)     7010050     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 4086, 32)     6432        embedding_2[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 4086, 64)     12864       embedding_2[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, 4086, 128)    25728       embedding_2[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 4086, 224)    0           conv1d_147[0][0]                 \n",
      "                                                                 conv1d_148[0][0]                 \n",
      "                                                                 conv1d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 1362, 224)    0           concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, 1360, 32)     21536       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_151 (Conv1D)             (None, 1360, 32)     21536       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_152 (Conv1D)             (None, 1360, 8)      5384        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 1360, 72)     0           conv1d_150[0][0]                 \n",
      "                                                                 conv1d_151[0][0]                 \n",
      "                                                                 conv1d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 1360, 72)     0           concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 453, 72)      0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_153 (Conv1D)             (None, 451, 64)      13888       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 28864)        0           conv1d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 28864)        0           flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 64)           1847360     dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 20)           1300        dense_42[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,966,078\n",
      "Trainable params: 1,956,028\n",
      "Non-trainable params: 7,010,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a20cc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/custom_callback\n",
    "from sklearn.metrics import f1_score\n",
    "class custom(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def on_train_begin(self,logs={}):\n",
    "    self.f1_score_list = []\n",
    "\n",
    "  def on_epoch_end(self,epoch,logs={}):\n",
    "\n",
    "    x_val,y_val = X_te5, y_te\n",
    "\n",
    "    pred_y = self.model.predict(x_val)\n",
    "\n",
    "    y_t = np.zeros(y_val.shape[0])\n",
    "    y_p = np.zeros(pred_y.shape[0])\n",
    "\n",
    "    for i in range(len(y_t)):\n",
    "      y_t[i] = int(np.argmax(y_val[i]))\n",
    "      y_p[i] = int(np.argmax(y_p[i]))\n",
    "\n",
    "    f1_value = f1_score(y_t,y_p,average=\"micro\")\n",
    "    print(\"f1_score:\",f1_value)\n",
    "\n",
    "    self.f1_score_list.append(f1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "011ae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "## f1_score_callback and other callbacks\n",
    "custom_callback = custom()\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_1.h5',verbose=2,monitor='val_accuracy', mode='max',save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor=\"val_accuracy\",mode='max',patience=6)\n",
    "\n",
    "## Tensorboard\n",
    "log_dir = \"logs\"\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "## compiling the model \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', F1Score(average='micro',num_classes=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d6cda7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "221/221 - 32s - loss: 1.0093 - accuracy: 0.7771 - f1_score: 0.7771 - val_loss: 1.2932 - val_accuracy: 0.6871 - val_f1_score: 0.6871\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.68536 to 0.68706, saving model to best_model_1.h5\n",
      "Epoch 2/25\n",
      "221/221 - 30s - loss: 0.9743 - accuracy: 0.7887 - f1_score: 0.7887 - val_loss: 1.2651 - val_accuracy: 0.6953 - val_f1_score: 0.6953\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.68706 to 0.69535, saving model to best_model_1.h5\n",
      "Epoch 3/25\n",
      "221/221 - 30s - loss: 0.9595 - accuracy: 0.7942 - f1_score: 0.7942 - val_loss: 1.2651 - val_accuracy: 0.7032 - val_f1_score: 0.7032\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.69535 to 0.70321, saving model to best_model_1.h5\n",
      "Epoch 4/25\n",
      "221/221 - 30s - loss: 0.9501 - accuracy: 0.7985 - f1_score: 0.7985 - val_loss: 1.2923 - val_accuracy: 0.6975 - val_f1_score: 0.6975\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.70321\n",
      "Epoch 5/25\n",
      "221/221 - 31s - loss: 0.9544 - accuracy: 0.7992 - f1_score: 0.7992 - val_loss: 1.2905 - val_accuracy: 0.6924 - val_f1_score: 0.6924\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.70321\n",
      "Epoch 6/25\n",
      "221/221 - 31s - loss: 0.9204 - accuracy: 0.8121 - f1_score: 0.8121 - val_loss: 1.3107 - val_accuracy: 0.6883 - val_f1_score: 0.6883\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.70321\n",
      "Epoch 7/25\n",
      "221/221 - 31s - loss: 0.8968 - accuracy: 0.8209 - f1_score: 0.8209 - val_loss: 1.2612 - val_accuracy: 0.7136 - val_f1_score: 0.7136\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.70321 to 0.71362, saving model to best_model_1.h5\n",
      "Epoch 8/25\n",
      "221/221 - 30s - loss: 0.8923 - accuracy: 0.8215 - f1_score: 0.8215 - val_loss: 1.2888 - val_accuracy: 0.7055 - val_f1_score: 0.7055\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.71362\n",
      "Epoch 9/25\n",
      "221/221 - 31s - loss: 0.8734 - accuracy: 0.8293 - f1_score: 0.8293 - val_loss: 1.2169 - val_accuracy: 0.7304 - val_f1_score: 0.7304\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.71362 to 0.73040, saving model to best_model_1.h5\n",
      "Epoch 10/25\n",
      "221/221 - 30s - loss: 0.8588 - accuracy: 0.8347 - f1_score: 0.8347 - val_loss: 1.3000 - val_accuracy: 0.7089 - val_f1_score: 0.7089\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.73040\n",
      "Epoch 11/25\n",
      "221/221 - 31s - loss: 0.8526 - accuracy: 0.8347 - f1_score: 0.8347 - val_loss: 1.2047 - val_accuracy: 0.7378 - val_f1_score: 0.7378\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.73040 to 0.73784, saving model to best_model_1.h5\n",
      "Epoch 12/25\n",
      "221/221 - 30s - loss: 0.8378 - accuracy: 0.8424 - f1_score: 0.8424 - val_loss: 1.2668 - val_accuracy: 0.7172 - val_f1_score: 0.7172\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.73784\n",
      "Epoch 13/25\n",
      "221/221 - 31s - loss: 0.8333 - accuracy: 0.8474 - f1_score: 0.8474 - val_loss: 1.2513 - val_accuracy: 0.7315 - val_f1_score: 0.7315\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.73784\n",
      "Epoch 14/25\n",
      "221/221 - 31s - loss: 0.8138 - accuracy: 0.8493 - f1_score: 0.8493 - val_loss: 1.2759 - val_accuracy: 0.7211 - val_f1_score: 0.7211\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.73784\n",
      "Epoch 15/25\n",
      "221/221 - 31s - loss: 0.8015 - accuracy: 0.8585 - f1_score: 0.8585 - val_loss: 1.4288 - val_accuracy: 0.6769 - val_f1_score: 0.6769\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.73784\n",
      "Epoch 16/25\n",
      "221/221 - 30s - loss: 0.8001 - accuracy: 0.8557 - f1_score: 0.8557 - val_loss: 1.2670 - val_accuracy: 0.7172 - val_f1_score: 0.7172\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.73784\n",
      "Epoch 17/25\n",
      "221/221 - 31s - loss: 0.7902 - accuracy: 0.8584 - f1_score: 0.8584 - val_loss: 1.2380 - val_accuracy: 0.7332 - val_f1_score: 0.7332\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.73784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f1ee35fc10>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainning \n",
    "model.fit(X_tr2,y_tr,epochs=25,verbose=2,validation_data=(X_te2,y_te),batch_size =64,callbacks=[checkpoint,early_stop,tensorboard]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f7fad723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./logs\n",
    "# !kill 26912"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075139e",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88d53f55",
   "metadata": {},
   "source": [
    "After preprocessing of given text data we got cleaned text as 'p_mail'(preprocessed mail),'p_text'(Preprocessed text),'p_subject'(preprocessed subjects).All three columns are combined as 'joined' which is used for modeling.\n",
    "    Joined data are then embedded and tokenized as instructed which converts characters to 50 dimension vectors.\n",
    "    Final data fetched to model1 i.e. using 1D convolutions with word embeddings and following are the results (after 17th epoch):-\n",
    "    1. Validation accuracy = 73.78%\n",
    "    2. Validation_F1_score = 73.32%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d2c87",
   "metadata": {},
   "source": [
    "## Model-2 : Using 1D convolutions with character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0c0ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr3, X_te3, y_tr3, y_te3 = train_test_split(X, y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3f2466b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr3 = X_tr3[0]\n",
    "X_te3 = X_te3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "97ae3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_char = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',char_level= True,oov_token='UNK')\n",
    "tokenize_char.fit_on_texts(X_tr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e65c23d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, ' ': 2, 'e': 3, 't': 4, 'a': 5, 'o': 6, 'n': 7, 'i': 8, 's': 9, 'r': 10, 'h': 11, 'l': 12, 'd': 13, 'c': 14, 'u': 15, 'm': 16, 'p': 17, 'g': 18, 'y': 19, 'w': 20, 'f': 21, 'b': 22, 'v': 23, 'k': 24, 'x': 25, 'j': 26, 'q': 27, 'z': 28, '_': 29, '1': 30, '0': 31, '2': 32, '-': 33, '3': 34, '4': 35, '6': 36, \"'\": 37, '5': 38, '8': 39, '9': 40, '7': 41, '|': 42, '\\\\': 43, '\\x08': 44, '\\x10': 45}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_char.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "49e2765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_char = len(tokenize_char.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9e601856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize them \n",
    "X_tr4 = tokenize_char.texts_to_sequences(X_tr3)\n",
    "X_te4 = tokenize_char.texts_to_sequences(X_te3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "24f29f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14121, 4089), (4707, 4089))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr5 = pad_sequences(X_tr4, maxlen=percentile_989, padding=\"post\")\n",
    "X_te5 = pad_sequences(X_te4, maxlen=percentile_989, padding=\"post\")\n",
    "X_tr5.shape, X_te5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "63e4c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## Make a embedding matrix \n",
    "emb_matrix_char = np.zeros((46,61))\n",
    "for i, j in tokenize_char.word_index.items():\n",
    "  emb_matrix_char[j][j] = 1\n",
    "print(emb_matrix_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "66a5e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding \n",
    "embedding_layer_char = Embedding(len(tokenize_char.word_index)+1,61, embeddings_initializer=tf.keras.initializers.Constant(emb_matrix_char),input_length=percentile_989,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b74688d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first layer\n",
    "first_layer = Input(shape=(percentile_989))\n",
    "\n",
    "embed = embedding_layer_char(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b9536361",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Conv1D(64,3,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(embed)\n",
    "n1 = Conv1D(64,3,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(m1)\n",
    "max_pool_1 = MaxPool1D(5)(n1)\n",
    "\n",
    "o1 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(max_pool_1)\n",
    "i1 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(o1)\n",
    "\n",
    "\n",
    "max_pool_2 = MaxPool1D(5)(i1)\n",
    "\n",
    "i1 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(max_pool_2)\n",
    "\n",
    "\n",
    "max_pool_3 = MaxPool1D(5)(i1)\n",
    "\n",
    "flatten = Flatten()(max_pool_3)\n",
    "\n",
    "dropout_layer = Dropout(0.5)(flatten)\n",
    "\n",
    "dense_layer1 = Dense(256,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42))(dropout_layer)\n",
    "\n",
    "\n",
    "              \n",
    "output_layer = Dense(20,activation=\"softmax\",kernel_initializer= tf.keras.initializers.glorot_normal(seed=42))(dense_layer1)\n",
    "              \n",
    "\n",
    "model2 = Model(inputs=first_layer,outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0bf649b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 4089)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 4089, 61)          2806      \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 4087, 64)          11776     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 4085, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 817, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 815, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 813, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 162, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 160, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 593,674\n",
      "Trainable params: 590,868\n",
      "Non-trainable params: 2,806\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7f88d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "## callbacks\n",
    "f1_call = custom()\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_1.h5',verbose=1,monitor='val_accuracy', mode='auto',save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor=\"val_accuracy\",mode='max',patience=2,verbose=1)\n",
    "\n",
    "## Tensorboard\n",
    "log_dir = \"logs\"\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "## reducing learning rate \n",
    "\n",
    "reduct = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',patience=1,mode='auto',verbose=1,factor=0.9)\n",
    "\n",
    "## compile model \n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy', F1Score(average='micro',num_classes=20)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b1276441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6/221 [..............................] - ETA: 1:45 - loss: 4.5612 - accuracy: 0.0651 - f1_score: 0.0651WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0534s vs `on_train_batch_end` time: 0.3768s). Check your callbacks.\n",
      "221/221 [==============================] - 34s 148ms/step - loss: 3.3339 - accuracy: 0.0797 - f1_score: 0.0797 - val_loss: 3.1538 - val_accuracy: 0.1160 - val_f1_score: 0.1160\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11600, saving model to best_model_1.h5\n",
      "Epoch 2/10\n",
      "221/221 [==============================] - 31s 138ms/step - loss: 3.0411 - accuracy: 0.1401 - f1_score: 0.1401 - val_loss: 2.9804 - val_accuracy: 0.1608 - val_f1_score: 0.1608\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.11600 to 0.16082, saving model to best_model_1.h5\n",
      "Epoch 3/10\n",
      "221/221 [==============================] - 31s 139ms/step - loss: 2.8512 - accuracy: 0.1918 - f1_score: 0.1918 - val_loss: 2.7723 - val_accuracy: 0.2144 - val_f1_score: 0.2144\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.16082 to 0.21436, saving model to best_model_1.h5\n",
      "Epoch 4/10\n",
      "221/221 [==============================] - 31s 138ms/step - loss: 2.6703 - accuracy: 0.2373 - f1_score: 0.2373 - val_loss: 2.6317 - val_accuracy: 0.2543 - val_f1_score: 0.2543\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.21436 to 0.25430, saving model to best_model_1.h5\n",
      "Epoch 5/10\n",
      "221/221 [==============================] - 31s 139ms/step - loss: 2.5250 - accuracy: 0.2803 - f1_score: 0.2803 - val_loss: 2.5254 - val_accuracy: 0.2781 - val_f1_score: 0.2781\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.25430 to 0.27810, saving model to best_model_1.h5\n",
      "Epoch 6/10\n",
      "221/221 [==============================] - 31s 139ms/step - loss: 2.4140 - accuracy: 0.3182 - f1_score: 0.3182 - val_loss: 2.5116 - val_accuracy: 0.2855 - val_f1_score: 0.2855\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.27810 to 0.28553, saving model to best_model_1.h5\n",
      "Epoch 7/10\n",
      "221/221 [==============================] - 31s 138ms/step - loss: 2.3045 - accuracy: 0.3460 - f1_score: 0.3460 - val_loss: 2.3215 - val_accuracy: 0.3614 - val_f1_score: 0.3614\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.28553 to 0.36138, saving model to best_model_1.h5\n",
      "Epoch 8/10\n",
      "221/221 [==============================] - 31s 139ms/step - loss: 2.2207 - accuracy: 0.3713 - f1_score: 0.3713 - val_loss: 2.2570 - val_accuracy: 0.3816 - val_f1_score: 0.3816\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.36138 to 0.38156, saving model to best_model_1.h5\n",
      "Epoch 9/10\n",
      "221/221 [==============================] - 31s 139ms/step - loss: 2.1481 - accuracy: 0.3946 - f1_score: 0.3946 - val_loss: 2.2678 - val_accuracy: 0.3605 - val_f1_score: 0.3605\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.999999772640876e-05.\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.38156\n",
      "Epoch 10/10\n",
      "221/221 [==============================] - 31s 140ms/step - loss: 2.0767 - accuracy: 0.4154 - f1_score: 0.4154 - val_loss: 2.1439 - val_accuracy: 0.4126 - val_f1_score: 0.4126\n",
      "f1_score: 0.04248990864669641\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.38156 to 0.41258, saving model to best_model_1.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f1f3946f10>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr5,y_tr3, epochs=10,validation_data=(X_te5, y_te3),batch_size =64,callbacks=[reduct , f1_call,checkpoint,early_stop,tensorboard]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7b48f",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "882bbc0f",
   "metadata": {},
   "source": [
    "Model2 i.e. 1D convolution using character embedding, following are the results (after 10th epoch):-\n",
    "    1. Validation_accuracy = 41.25%\n",
    "    2. Validation_F1_score = 41.26%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2b7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
